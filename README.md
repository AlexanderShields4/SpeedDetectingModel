# MemePoseMatcher

A fun, real-time application that detects your upper-body pose and facial expressions using MediaPipe, classifies them into three meme-driven expressions, and displays matching 3-second video clips in a GUI overlay.

## Features

- **Real-time detection** using MediaPipe Pose and Face Mesh
- **Expression classification** into three meme categories:
  - ðŸ—£ï¸ **messi_yell**: Wide-open mouth (yelling/shouting)
  - ðŸ˜Š **mom_homeless_laugh**: Eyes closed, laughing expression
  - ðŸ• **bark_lilnasx**: Energetic arm/head movement
- **Neutral state**: When no strong expression is detected, the app continues to loop the last matched clip
- **Live metrics display**: See real-time values for eye aspect ratio (EAR) and mouth aspect ratio (MAR)
- **Smooth classification** with temporal windowing and debouncing to avoid jitter

## Setup

### 1. Clone/Create the Project

```bash
cd /home/alex/projects/MachineLearningMeme
```

### 2. Create a Virtual Environment

```bash
python3 -m venv venv
source venv/bin/activate
```

### 3. Install Dependencies

```bash
pip install -r requirements.txt
```

### 4. Add Your Meme Video Clips

Place your three 3-second video files into the `data/videos/` directory:

- `speed_messi.mp4` - for the yelling/wide-mouth expression
- `speed_mom_homeless.mp4` - for the laughing expression
- `speed_bark_lilnasx.mp4` - for the energetic movement

**Example**:
```bash
ls data/videos/
# Output:
# speed_bark_lilnasx.mp4
# speed_mom_homeless.mp4
# speed_messi.mp4
```

> **Note**: If videos are missing, the app will still run but won't display overlays. You'll see a warning at startup.

## Running the App

```bash
python -m src.app
```

You should see:
1. A large window showing your webcam feed with pose/face skeleton overlays
2. A small video panel in the top-left corner playing the matched expression clip
3. Real-time metrics (EAR, MAR, arm extension) displayed on screen
4. Console output with frame statistics and expression labels

## Controls

- **`q`**: Quit the application
- **`m`**: Toggle metrics display on/off

## Customization

### Tuning Classifier Thresholds

Edit the thresholds in `src/classifier.py`:

```python
self.MAR_YELL_THRESHOLD = 0.45       # increase for harder yelling detection
self.EAR_CLOSED_THRESHOLD = 0.18     # adjust eye-closed sensitivity
self.BARK_HEAD_MOVEMENT = 15.0       # pixels of head oscillation
```

### Changing Video Panel Size

In `src/app.py`, modify the `VideoPlayer` initialization:

```python
video_player = VideoPlayer(label_to_path=label_to_path, panel_size=(640, 480))  # larger panel
```

### Changing Webcam Resolution

In `src/app.py`:

```python
capture = MediaPipeCapture(cam_index=0, webcam_width=1920, webcam_height=1080)
```

## Troubleshooting

- **Webcam not detected**: Check that your camera is connected and not in use by another application. Try `cam_index=1` or `cam_index=2` in `src/app.py`.
- **Poor pose/face detection**: Ensure good lighting and that you're facing the camera.
- **Videos not playing**: Verify that video files are in `data/videos/` and have the correct filenames.
- **Expression not classifying correctly**: Adjust the thresholds in `src/classifier.py` and try exaggerating your expression.

## Project Structure

```
meme_pose_matcher/
â”œâ”€â”€ data/
â”‚   â””â”€â”€ videos/                     # Your three video files go here
â”œâ”€â”€ requirements.txt                 # Dependencies
â”œâ”€â”€ README.md                        # This file
â””â”€â”€ src/
    â”œâ”€â”€ __init__.py                 # Package marker
    â”œâ”€â”€ capture.py                  # MediaPipe webcam + landmark capture
    â”œâ”€â”€ pose_utils.py               # Landmark processing & metrics (EAR, MAR, angles)
    â”œâ”€â”€ classifier.py               # Expression classification logic (rule-based)
    â”œâ”€â”€ video_player.py             # Video clip playback manager
    â”œâ”€â”€ ui.py                       # OpenCV GUI and overlay rendering
    â””â”€â”€ app.py                      # Main application entry point
```

## How It Works

1. **Capture**: MediaPipe Pose and Face Mesh detect landmarks from your webcam feed (33 pose points, 468 face points).
2. **Metrics**: Eye aspect ratio (EAR), mouth aspect ratio (MAR), arm extension, and head movement are calculated.
3. **Classification**: A rule-based classifier examines these metrics over a 0.6-second temporal window and classifies into one of four states: `messi_yell`, `mom_homeless_laugh`, `bark_lilnasx`, or `neutral`.
4. **Debouncing**: To avoid rapid label jitter, the classifier requires a 0.25-second hold before switching to a new expression.
5. **Playback**: The `VideoPlayer` displays the matched clip (or repeats the last active clip during neutral states).
6. **UI**: OpenCV window shows the camera feed with skeleton overlays, live metrics, and the video panel.

## Future Enhancements

- Train a small neural net classifier for more robust expression detection
- Add support for more expressions or custom meme clips
- Implement eye-gaze tracking for additional metrics
- Add sound (play audio tracks alongside video)
- Support multiple simultaneous detection (detect multiple faces)
- Add pose confidence/validity filtering

## License

This project is provided as-is for fun and educational purposes.

---

**Enjoy making memes with your own expressions!** ðŸŽ¬ðŸ˜„
